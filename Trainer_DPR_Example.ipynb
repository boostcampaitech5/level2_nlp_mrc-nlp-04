{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "from datasets import load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset, SequentialSampler\n",
    "from input.code.dpr.trainer_DPR import BiEncoderTrainer # trainer_DPR 모듈 위치에 따라서, from을 수정해주세요\n",
    "from input.code.dpr.cls_Encoder import BertEncoder, RoBertaEncoder # cls_Encoder 모듈 위치에 따라서, from을 수정해주세요\n",
    "from input.code.bm25 import make_bm25_embedding # bm25 모듈 위치에 따라서, from을 수정해주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BASE_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"input\", \"data\")\n",
    "P_ENCODER_DIR = os.path.join(BASE_DIR, \"input\", \"code\", \"dpr\", \"roberta\",\"p_encoder\")\n",
    "Q_ENCODER_DIR = os.path.join(BASE_DIR, \"input\", \"code\", \"dpr\", \"roberta\", \"q_encoder\")\n",
    "datasets = load_from_disk(os.path.join(DATA_DIR, \"train_dataset\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_DIR, \"wikipedia_documents.json\"), \"r\") as f:\n",
    "\twiki_corpus = json.load(f)\n",
    "wiki_corpus = list(dict.fromkeys([v['text'] for v in wiki_corpus.values()]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_name = \"klue/roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "p_encoder = BertEncoder.from_pretrained(model_name).to(\"cuda:0\")\n",
    "q_encoder = BertEncoder.from_pretrained(model_name).to(\"cuda:0\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bitrainer = BiEncoderTrainer(p_encoder=p_encoder,\n",
    "\t\t\t\t\t\t\t q_encoder=q_encoder,\n",
    "\t\t\t\t\t\t\t tokenizer=tokenizer,\n",
    "\t\t\t\t\t\t\t epochs=3,\n",
    "\t\t\t\t\t\t\t batch_size=30,\n",
    "\t\t\t\t\t\t\t neg_num=2,\n",
    "\t\t\t\t\t\t\t lr=5e-5,\n",
    "\t\t\t\t\t\t\t train_datasets=datasets['train'],\n",
    "\t\t\t\t\t\t\t eval_datasets=datasets['validation'],\n",
    "\t\t\t\t\t\t\t contexts_document=wiki_corpus,)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bitrainer.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample_idx = np.random.choice(range(len(datasets['validation'])), 5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "passage_dataset = datasets['validation'][sample_idx]\n",
    "query = datasets['validation'][sample_idx]['question']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "score, pred_rank, pred_corpus = bitrainer.predict(passage_dataset, query)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "\tprint(\"[Query] : \", query[i])\n",
    "\tprint(\"[True Passage] \\n\", passage_dataset['context'][i])\n",
    "\tfor k in range(5):\n",
    "\t\tprint(f\"Top-{k+1} Passage\")\n",
    "\t\tprint(\"[Score] : \", score[i][k])\n",
    "\t\tprint(\"[Predicted Passage] \\n\", pred_corpus[i][k])\n",
    "\t\tprint(\"=\" * 15)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BM25 TEST Dataset Rerank with DPR"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wiki_corpus = np.array(wiki_corpus)\n",
    "test_dataset = load_from_disk(os.path.join(DATA_DIR, \"test_dataset\"))\n",
    "test_dataset = test_dataset['validation']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if os.path.exists(os.path.join(DATA_DIR, \"test_wiki_bm25_embedding.bin\")):\n",
    "\twith open(os.path.join(DATA_DIR, \"test_wiki_bm25_embedding.bin\" ), \"rb\") as f:\n",
    "\t\tbm25_embedding = pickle.load(f)\n",
    "else:\n",
    "\tprint(\"BM25 embedding file not exists\")\n",
    "\tbm25_embedding = make_bm25_embedding(DATA_DIR=DATA_DIR, tokenizer=tokenizer, full_ds=test_dataset, context=wiki_corpus)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "top_k_idx = np.argsort(bm25_embedding[:, :])[:, ::-1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "q_seqs = tokenizer(test_dataset['question'],\n",
    "\t\t\t\t\tpadding=\"max_length\",\n",
    "\t\t\t\t\ttruncation=True,\n",
    "\t\t\t\t   max_length=50,\n",
    "\t\t\t\t\treturn_tensors=\"pt\").to(\"cuda:0\")\n",
    "q_test = TensorDataset(q_seqs['input_ids'],\n",
    "\t\t\t\t\t   q_seqs['attention_mask'],\n",
    "\t\t\t\t\t   q_seqs['token_type_ids'])\n",
    "q_sampler = SequentialSampler(q_test)\n",
    "q_dataloader = DataLoader(q_test, batch_size=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "query_per_score = []\n",
    "top_k = 50\n",
    "with torch.no_grad():\n",
    "\tq_encoder.eval()\n",
    "\tp_encoder.eval()\n",
    "\tepoch_iter = tqdm(test_dataset['question'], desc=f\"Top-{top_k} Question Per Wiki Passage\")\n",
    "\n",
    "\tfor i, q_seqs in enumerate(epoch_iter):\n",
    "\t\tq_inputs = tokenizer(q_seqs,\n",
    "\t\t\t\t\t\t\t padding=\"max_length\",\n",
    "\t\t\t\t\t\t\t truncation=True,\n",
    "\t\t\t\t\t\t\t max_length=50,\n",
    "\t\t\t\t\t\t\t return_tensors=\"pt\").to(\"cuda:0\")\n",
    "\t\tp_inputs = tokenizer(wiki_corpus[top_k_idx[i][:top_k]].tolist(),\n",
    "\t\t\t\t\t\t\t padding=\"max_length\",\n",
    "\t\t\t\t\t\t\t truncation=True,\n",
    "\t\t\t\t\t\t\t max_length=500,\n",
    "\t\t\t\t\t\t\t return_tensors=\"pt\").to(\"cuda:0\")\n",
    "\n",
    "\n",
    "\t\tq_outputs = q_encoder(**q_inputs).to('cpu')\n",
    "\t\tp_outputs = p_encoder(**p_inputs).to('cpu')\n",
    "\t\tscore = torch.matmul(q_outputs, p_outputs.T).tolist()\n",
    "\t\tquery_per_score.append(score)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rerank_corpus = [wiki_corpus[top_k_idx[i, np.argsort(query_per_score[i])[::-1]]][0].tolist() for i in range(len(query_per_score))]\n",
    "with open(os.path.join(DATA_DIR, \"rerank_corpus.bin\" ), \"wb\") as f:\n",
    "\tpickle.dump(rerank_corpus, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
